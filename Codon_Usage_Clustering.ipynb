{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nasywaanaa/ML-CodonAnalysis/blob/main/Codon_Usage_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "8nc6f-gD7LmR"
      },
      "outputs": [],
      "source": [
        "# Enhanced Codon Clustering and Classification Pipeline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
        "from scipy.stats import entropy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.manifold import TSNE\n",
        "import umap.umap_ as umap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_samples\n",
        "import matplotlib.cm as cm\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import xgboost as xgb\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import os # Import os module for file path operations\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File ID from Google Drive\n",
        "# https://drive.google.com/file/d/1QtwXZrkSKBiO31Btp1AJuPaIX3FUvO5r/view?usp=drive_link => File ID: 1QtwXZrkSKBiO31Btp1AJuPaIX3FUvO5r\n",
        "# Penggunaan format berikut:\n",
        "# https://drive.google.com/uc?id={file_id}\n",
        "\n",
        "# Read File CSV\n",
        "codon_data = pd.read_csv('https://drive.google.com/uc?id=1QtwXZrkSKBiO31Btp1AJuPaIX3FUvO5r',sep=',', low_memory=False)\n",
        "codon_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "CJwbxpgW7Nzv",
        "outputId": "299b1c88-417b-4cbc-cf53-5c25c203fb51"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Kingdom  DNAtype  SpeciesID   Ncodons  \\\n",
              "0         vrl        0     100217      1995   \n",
              "1         vrl        0     100220      1474   \n",
              "2         vrl        0     100755      4862   \n",
              "3         vrl        0     100880      1915   \n",
              "4         vrl        0     100887     22831   \n",
              "...       ...      ...        ...       ...   \n",
              "13023     pri        0       9601      1097   \n",
              "13024     pri        1       9601      2067   \n",
              "13025     pri        1       9602      1686   \n",
              "13026     pri        0       9606  40662582   \n",
              "13027     pri        1       9606   8998998   \n",
              "\n",
              "                                   SpeciesName      UUU      UUC      UUA  \\\n",
              "0      Epizootic haematopoietic necrosis virus  0.01654  0.01203  0.00050   \n",
              "1                             Bohle iridovirus  0.02714  0.01357  0.00068   \n",
              "2                 Sweet potato leaf curl virus  0.01974   0.0218  0.01357   \n",
              "3                 Northern cereal mosaic virus  0.01775  0.02245  0.01619   \n",
              "4               Soil-borne cereal mosaic virus  0.02816  0.01371  0.00767   \n",
              "...                                        ...      ...      ...      ...   \n",
              "13023                    Pongo pygmaeus abelii  0.02552  0.03555  0.00547   \n",
              "13024      mitochondrion Pongo pygmaeus abelii  0.01258  0.03193  0.01984   \n",
              "13025    mitochondrion Pongo pygmaeus pygmaeus  0.01423  0.03321  0.01661   \n",
              "13026                             Homo sapiens  0.01757  0.02028  0.00767   \n",
              "13027               mitochondrion Homo sapiens  0.01778  0.03724  0.01732   \n",
              "\n",
              "           UUG      CUU  ...      CGG      AGA      AGG      GAU      GAC  \\\n",
              "0      0.00351  0.01203  ...  0.00451  0.01303  0.03559  0.01003  0.04612   \n",
              "1      0.00678  0.00407  ...  0.00136  0.01696  0.03596  0.01221  0.04545   \n",
              "2      0.01543  0.00782  ...  0.00596  0.01974  0.02489  0.03126  0.02036   \n",
              "3      0.00992  0.01567  ...  0.00366  0.01410  0.01671  0.03760  0.01932   \n",
              "4      0.03679  0.01380  ...  0.00604  0.01494  0.01734  0.04148  0.02483   \n",
              "...        ...      ...  ...      ...      ...      ...      ...      ...   \n",
              "13023  0.01367  0.01276  ...  0.00820  0.01367  0.01094  0.01367  0.02279   \n",
              "13024  0.00629  0.01451  ...  0.00145  0.00000  0.00048  0.00194  0.01306   \n",
              "13025  0.00356  0.01127  ...  0.00000  0.00000  0.00000  0.00178  0.01661   \n",
              "13026  0.01293  0.01319  ...  0.01142  0.01217  0.01196  0.02178  0.02510   \n",
              "13027  0.00600  0.01689  ...  0.00083  0.00041  0.00041  0.00451  0.01402   \n",
              "\n",
              "           GAA      GAG      UAA      UAG      UGA  \n",
              "0      0.01203  0.04361  0.00251  0.00050  0.00000  \n",
              "1      0.01560  0.04410  0.00271  0.00068  0.00000  \n",
              "2      0.02242  0.02468  0.00391  0.00000  0.00144  \n",
              "3      0.03029  0.03446  0.00261  0.00157  0.00000  \n",
              "4      0.03359  0.03679  0.00000  0.00044  0.00131  \n",
              "...        ...      ...      ...      ...      ...  \n",
              "13023  0.02005  0.04102  0.00091  0.00091  0.00638  \n",
              "13024  0.01838  0.00677  0.00242  0.00097  0.01887  \n",
              "13025  0.02788  0.00297  0.00356  0.00119  0.02017  \n",
              "13026  0.02896  0.03959  0.00099  0.00079  0.00156  \n",
              "13027  0.01651  0.00783  0.00156  0.00114  0.02161  \n",
              "\n",
              "[13028 rows x 69 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3f1d899f-38c1-4be7-9f30-ae2319cb59dc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kingdom</th>\n",
              "      <th>DNAtype</th>\n",
              "      <th>SpeciesID</th>\n",
              "      <th>Ncodons</th>\n",
              "      <th>SpeciesName</th>\n",
              "      <th>UUU</th>\n",
              "      <th>UUC</th>\n",
              "      <th>UUA</th>\n",
              "      <th>UUG</th>\n",
              "      <th>CUU</th>\n",
              "      <th>...</th>\n",
              "      <th>CGG</th>\n",
              "      <th>AGA</th>\n",
              "      <th>AGG</th>\n",
              "      <th>GAU</th>\n",
              "      <th>GAC</th>\n",
              "      <th>GAA</th>\n",
              "      <th>GAG</th>\n",
              "      <th>UAA</th>\n",
              "      <th>UAG</th>\n",
              "      <th>UGA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>vrl</td>\n",
              "      <td>0</td>\n",
              "      <td>100217</td>\n",
              "      <td>1995</td>\n",
              "      <td>Epizootic haematopoietic necrosis virus</td>\n",
              "      <td>0.01654</td>\n",
              "      <td>0.01203</td>\n",
              "      <td>0.00050</td>\n",
              "      <td>0.00351</td>\n",
              "      <td>0.01203</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00451</td>\n",
              "      <td>0.01303</td>\n",
              "      <td>0.03559</td>\n",
              "      <td>0.01003</td>\n",
              "      <td>0.04612</td>\n",
              "      <td>0.01203</td>\n",
              "      <td>0.04361</td>\n",
              "      <td>0.00251</td>\n",
              "      <td>0.00050</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>vrl</td>\n",
              "      <td>0</td>\n",
              "      <td>100220</td>\n",
              "      <td>1474</td>\n",
              "      <td>Bohle iridovirus</td>\n",
              "      <td>0.02714</td>\n",
              "      <td>0.01357</td>\n",
              "      <td>0.00068</td>\n",
              "      <td>0.00678</td>\n",
              "      <td>0.00407</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00136</td>\n",
              "      <td>0.01696</td>\n",
              "      <td>0.03596</td>\n",
              "      <td>0.01221</td>\n",
              "      <td>0.04545</td>\n",
              "      <td>0.01560</td>\n",
              "      <td>0.04410</td>\n",
              "      <td>0.00271</td>\n",
              "      <td>0.00068</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>vrl</td>\n",
              "      <td>0</td>\n",
              "      <td>100755</td>\n",
              "      <td>4862</td>\n",
              "      <td>Sweet potato leaf curl virus</td>\n",
              "      <td>0.01974</td>\n",
              "      <td>0.0218</td>\n",
              "      <td>0.01357</td>\n",
              "      <td>0.01543</td>\n",
              "      <td>0.00782</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00596</td>\n",
              "      <td>0.01974</td>\n",
              "      <td>0.02489</td>\n",
              "      <td>0.03126</td>\n",
              "      <td>0.02036</td>\n",
              "      <td>0.02242</td>\n",
              "      <td>0.02468</td>\n",
              "      <td>0.00391</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>vrl</td>\n",
              "      <td>0</td>\n",
              "      <td>100880</td>\n",
              "      <td>1915</td>\n",
              "      <td>Northern cereal mosaic virus</td>\n",
              "      <td>0.01775</td>\n",
              "      <td>0.02245</td>\n",
              "      <td>0.01619</td>\n",
              "      <td>0.00992</td>\n",
              "      <td>0.01567</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00366</td>\n",
              "      <td>0.01410</td>\n",
              "      <td>0.01671</td>\n",
              "      <td>0.03760</td>\n",
              "      <td>0.01932</td>\n",
              "      <td>0.03029</td>\n",
              "      <td>0.03446</td>\n",
              "      <td>0.00261</td>\n",
              "      <td>0.00157</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>vrl</td>\n",
              "      <td>0</td>\n",
              "      <td>100887</td>\n",
              "      <td>22831</td>\n",
              "      <td>Soil-borne cereal mosaic virus</td>\n",
              "      <td>0.02816</td>\n",
              "      <td>0.01371</td>\n",
              "      <td>0.00767</td>\n",
              "      <td>0.03679</td>\n",
              "      <td>0.01380</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00604</td>\n",
              "      <td>0.01494</td>\n",
              "      <td>0.01734</td>\n",
              "      <td>0.04148</td>\n",
              "      <td>0.02483</td>\n",
              "      <td>0.03359</td>\n",
              "      <td>0.03679</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00044</td>\n",
              "      <td>0.00131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13023</th>\n",
              "      <td>pri</td>\n",
              "      <td>0</td>\n",
              "      <td>9601</td>\n",
              "      <td>1097</td>\n",
              "      <td>Pongo pygmaeus abelii</td>\n",
              "      <td>0.02552</td>\n",
              "      <td>0.03555</td>\n",
              "      <td>0.00547</td>\n",
              "      <td>0.01367</td>\n",
              "      <td>0.01276</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00820</td>\n",
              "      <td>0.01367</td>\n",
              "      <td>0.01094</td>\n",
              "      <td>0.01367</td>\n",
              "      <td>0.02279</td>\n",
              "      <td>0.02005</td>\n",
              "      <td>0.04102</td>\n",
              "      <td>0.00091</td>\n",
              "      <td>0.00091</td>\n",
              "      <td>0.00638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13024</th>\n",
              "      <td>pri</td>\n",
              "      <td>1</td>\n",
              "      <td>9601</td>\n",
              "      <td>2067</td>\n",
              "      <td>mitochondrion Pongo pygmaeus abelii</td>\n",
              "      <td>0.01258</td>\n",
              "      <td>0.03193</td>\n",
              "      <td>0.01984</td>\n",
              "      <td>0.00629</td>\n",
              "      <td>0.01451</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00145</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00048</td>\n",
              "      <td>0.00194</td>\n",
              "      <td>0.01306</td>\n",
              "      <td>0.01838</td>\n",
              "      <td>0.00677</td>\n",
              "      <td>0.00242</td>\n",
              "      <td>0.00097</td>\n",
              "      <td>0.01887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13025</th>\n",
              "      <td>pri</td>\n",
              "      <td>1</td>\n",
              "      <td>9602</td>\n",
              "      <td>1686</td>\n",
              "      <td>mitochondrion Pongo pygmaeus pygmaeus</td>\n",
              "      <td>0.01423</td>\n",
              "      <td>0.03321</td>\n",
              "      <td>0.01661</td>\n",
              "      <td>0.00356</td>\n",
              "      <td>0.01127</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00178</td>\n",
              "      <td>0.01661</td>\n",
              "      <td>0.02788</td>\n",
              "      <td>0.00297</td>\n",
              "      <td>0.00356</td>\n",
              "      <td>0.00119</td>\n",
              "      <td>0.02017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13026</th>\n",
              "      <td>pri</td>\n",
              "      <td>0</td>\n",
              "      <td>9606</td>\n",
              "      <td>40662582</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>0.01757</td>\n",
              "      <td>0.02028</td>\n",
              "      <td>0.00767</td>\n",
              "      <td>0.01293</td>\n",
              "      <td>0.01319</td>\n",
              "      <td>...</td>\n",
              "      <td>0.01142</td>\n",
              "      <td>0.01217</td>\n",
              "      <td>0.01196</td>\n",
              "      <td>0.02178</td>\n",
              "      <td>0.02510</td>\n",
              "      <td>0.02896</td>\n",
              "      <td>0.03959</td>\n",
              "      <td>0.00099</td>\n",
              "      <td>0.00079</td>\n",
              "      <td>0.00156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13027</th>\n",
              "      <td>pri</td>\n",
              "      <td>1</td>\n",
              "      <td>9606</td>\n",
              "      <td>8998998</td>\n",
              "      <td>mitochondrion Homo sapiens</td>\n",
              "      <td>0.01778</td>\n",
              "      <td>0.03724</td>\n",
              "      <td>0.01732</td>\n",
              "      <td>0.00600</td>\n",
              "      <td>0.01689</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00083</td>\n",
              "      <td>0.00041</td>\n",
              "      <td>0.00041</td>\n",
              "      <td>0.00451</td>\n",
              "      <td>0.01402</td>\n",
              "      <td>0.01651</td>\n",
              "      <td>0.00783</td>\n",
              "      <td>0.00156</td>\n",
              "      <td>0.00114</td>\n",
              "      <td>0.02161</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13028 rows × 69 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f1d899f-38c1-4be7-9f30-ae2319cb59dc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3f1d899f-38c1-4be7-9f30-ae2319cb59dc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3f1d899f-38c1-4be7-9f30-ae2319cb59dc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f15dc27c-d93f-402d-89e7-a44437827566\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f15dc27c-d93f-402d-89e7-a44437827566')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f15dc27c-d93f-402d-89e7-a44437827566 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_03edc7cf-c6d8-48e6-bf00-440255cda3e9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('codon_data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_03edc7cf-c6d8-48e6-bf00-440255cda3e9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('codon_data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "codon_data"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedCodonClusteringPipeline:\n",
        "    def __init__(self, codon_data):\n",
        "        self.codon_data = codon_data.copy()\n",
        "        self.codon_cols = codon_data.columns[5:]\n",
        "        self.codon_to_aa = {\n",
        "            'GCU': 'A', 'GCC': 'A', 'GCA': 'A', 'GCG': 'A',\n",
        "            'CGU': 'R', 'CGC': 'R', 'CGA': 'R', 'CGG': 'R', 'AGA': 'R', 'AGG': 'R',\n",
        "            'GGU': 'G', 'GGC': 'G', 'GGA': 'G', 'GGG': 'G',\n",
        "            'AAA': 'K', 'AAG': 'K',\n",
        "            'UUU': 'F', 'UUC': 'F',\n",
        "            'CCU': 'P', 'CCC': 'P', 'CCA': 'P', 'CCG': 'P',\n",
        "            'UCU': 'S', 'UCC': 'S', 'UCA': 'S', 'UCG': 'S', 'AGU': 'S', 'AGC': 'S',\n",
        "            'AUU': 'I', 'AUC': 'I', 'AUA': 'I',\n",
        "            'AUG': 'M',\n",
        "            'GUU': 'V', 'GUC': 'V', 'GUA': 'V', 'GUG': 'V',\n",
        "            'UUA': 'L', 'UUG': 'L', 'CUU': 'L', 'CUC': 'L', 'CUA': 'L', 'CUG': 'L',\n",
        "            'ACU': 'T', 'ACC': 'T', 'ACA': 'T', 'ACG': 'T',\n",
        "            'UAU': 'Y', 'UAC': 'Y',\n",
        "            'UAA': 'Stop', 'UAG': 'Stop', 'UGA': 'Stop',\n",
        "            'CAA': 'Q', 'CAG': 'Q',\n",
        "            'AAU': 'N', 'AAC': 'N',\n",
        "            'GAU': 'D', 'GAC': 'D',\n",
        "            'UGU': 'C', 'UGC': 'C',\n",
        "            'GAA': 'E', 'GAG': 'E',\n",
        "            'CAU': 'H', 'CAC': 'H',\n",
        "            'UGG': 'W'  # Added missing Tryptophan\n",
        "        }\n",
        "\n",
        "        # Group amino acids by properties for better feature engineering\n",
        "        self.aa_properties = {\n",
        "            'hydrophobic': ['A', 'V', 'I', 'L', 'M', 'F', 'Y', 'W'],\n",
        "            'polar': ['S', 'T', 'N', 'Q', 'C'],\n",
        "            'charged_positive': ['K', 'R', 'H'],\n",
        "            'charged_negative': ['D', 'E'],\n",
        "            'special': ['G', 'P']\n",
        "        }\n",
        "\n",
        "        self.features_log = None\n",
        "        self.X_pca = None\n",
        "        self.X_scaled = None\n",
        "        self.outliers_mask = None\n",
        "\n",
        "    def clean_data(self, fill_na=True, remove_outliers=True):\n",
        "        \"\"\"Enhanced data cleaning with outlier detection\"\"\"\n",
        "        # Duplicate check\n",
        "        dup_count = self.codon_data.duplicated().sum()\n",
        "        print(f\"Duplicate Rows: {dup_count}\")\n",
        "\n",
        "        # Remove duplicates\n",
        "        self.codon_data = self.codon_data.drop_duplicates()\n",
        "\n",
        "        # Convert to numeric & fill missing\n",
        "        for col in self.codon_cols:\n",
        "            self.codon_data[col] = pd.to_numeric(self.codon_data[col], errors='coerce')\n",
        "\n",
        "        if fill_na:\n",
        "            # Use median instead of mean for better outlier handling\n",
        "            numeric_cols = self.codon_data.select_dtypes(include=np.number).columns\n",
        "            self.codon_data[numeric_cols] = self.codon_data[numeric_cols].fillna(\n",
        "                self.codon_data[numeric_cols].median()\n",
        "            )\n",
        "\n",
        "        # Remove rows with all zeros (likely corrupted data)\n",
        "        codon_sum = self.codon_data[self.codon_cols].sum(axis=1)\n",
        "        self.codon_data = self.codon_data[codon_sum > 0]\n",
        "\n",
        "        print(f\"Data shape after cleaning: {self.codon_data.shape}\")\n",
        "        return self\n",
        "\n",
        "    def aggregate_codon_to_aa(self):\n",
        "        \"\"\"Enhanced aggregation with better handling\"\"\"\n",
        "        aa_features = {}\n",
        "        for codon, aa in self.codon_to_aa.items():\n",
        "            if aa == 'Stop':\n",
        "                continue\n",
        "            if codon in self.codon_cols:\n",
        "                aa_features.setdefault(aa, []).append(self.codon_data[codon])\n",
        "\n",
        "        df_aa = pd.DataFrame()\n",
        "        for aa, cols in aa_features.items():\n",
        "            df_aa[aa] = np.sum(cols, axis=0)\n",
        "\n",
        "        self.df_aa = df_aa\n",
        "        return self\n",
        "\n",
        "    def normalize_per_row(self):\n",
        "        \"\"\"Enhanced normalization with pseudocount\"\"\"\n",
        "        # Add pseudocount to avoid division by zero and log(0)\n",
        "        pseudocount = 1e-8\n",
        "        row_sums = self.df_aa.sum(axis=1)\n",
        "\n",
        "        # Filter out rows with very low counts (likely noise)\n",
        "        valid_rows = row_sums > row_sums.quantile(0.05)  # Remove bottom 5%\n",
        "        self.df_aa = self.df_aa[valid_rows]\n",
        "        self.codon_data = self.codon_data[valid_rows]\n",
        "\n",
        "        row_sums = self.df_aa.sum(axis=1)\n",
        "        self.df_aa_norm = (self.df_aa + pseudocount).div(row_sums + pseudocount * len(self.df_aa.columns), axis=0)\n",
        "\n",
        "        print(f\"Data shape after filtering low-count rows: {self.df_aa_norm.shape}\")\n",
        "        return self\n",
        "\n",
        "    def add_advanced_features(self):\n",
        "        \"\"\"Add more sophisticated features with robust NaN handling\"\"\"\n",
        "        df = self.df_aa_norm\n",
        "        stats = pd.DataFrame(index=df.index)\n",
        "\n",
        "        # Basic statistics with NaN handling\n",
        "        stats['mean'] = df.mean(axis=1)\n",
        "        stats['median'] = df.median(axis=1)\n",
        "        stats['std'] = df.std(axis=1).fillna(0)  # Fill NaN std with 0\n",
        "        stats['skewness'] = df.skew(axis=1).fillna(0)  # Fill NaN skewness with 0\n",
        "        stats['kurtosis'] = df.kurtosis(axis=1).fillna(0)  # Fill NaN kurtosis with 0\n",
        "\n",
        "        # Entropy with better handling\n",
        "        freq_norm = df.div(df.sum(axis=1), axis=0).fillna(0)\n",
        "        stats['entropy'] = freq_norm.apply(lambda x: entropy(x + 1e-10, base=2), axis=1)\n",
        "        stats['entropy'] = stats['entropy'].fillna(0)  # Fill NaN entropy with 0\n",
        "\n",
        "        # Gini coefficient (measure of inequality) with NaN handling\n",
        "        def gini_coefficient(x):\n",
        "            try:\n",
        "                x = np.array(x)\n",
        "                x = x[~np.isnan(x)]  # Remove NaN values\n",
        "                if len(x) == 0 or np.sum(x) == 0:\n",
        "                    return 0\n",
        "                x = np.sort(x)\n",
        "                n = len(x)\n",
        "                index = np.arange(1, n + 1)\n",
        "                return (2 * np.sum(index * x)) / (n * np.sum(x)) - (n + 1) / n\n",
        "            except:\n",
        "                return 0\n",
        "\n",
        "        stats['gini'] = df.apply(gini_coefficient, axis=1)\n",
        "\n",
        "        # Amino acid property-based features with NaN handling\n",
        "        for prop_name, aa_list in self.aa_properties.items():\n",
        "            prop_cols = [aa for aa in aa_list if aa in df.columns]\n",
        "            if prop_cols:\n",
        "                stats[f'{prop_name}_usage'] = df[prop_cols].sum(axis=1)\n",
        "                prop_entropy = df[prop_cols].apply(\n",
        "                    lambda x: entropy(x + 1e-10, base=2) if len(x) > 0 and np.sum(x) > 0 else 0, axis=1\n",
        "                )\n",
        "                stats[f'{prop_name}_entropy'] = prop_entropy.fillna(0)\n",
        "\n",
        "        # Codon usage bias indices\n",
        "        self.calculate_codon_bias_indices(df, stats)\n",
        "\n",
        "        # Final NaN check and fill\n",
        "        stats = stats.fillna(0)\n",
        "\n",
        "        self.stats = stats\n",
        "        return self\n",
        "\n",
        "    def calculate_codon_bias_indices(self, df, stats):\n",
        "        \"\"\"Calculate advanced codon usage bias indices with robust NaN handling\"\"\"\n",
        "        # Effective Number of Codons (ENc) - approximation\n",
        "        nc_values = []\n",
        "        for idx in df.index:\n",
        "            try:\n",
        "                aa_usage = df.loc[idx]\n",
        "                # Simple ENc approximation\n",
        "                nc = 0\n",
        "                for aa_group in self.aa_properties.values():\n",
        "                    group_usage = [aa_usage.get(aa, 0) for aa in aa_group if aa in aa_usage.index]\n",
        "                    if group_usage and sum(group_usage) > 0:\n",
        "                        group_usage = np.array(group_usage)\n",
        "                        group_usage = group_usage[~np.isnan(group_usage)]  # Remove NaN\n",
        "                        if len(group_usage) > 0 and np.sum(group_usage) > 0:\n",
        "                            group_usage = group_usage / group_usage.sum()\n",
        "                            nc += 1 / max(np.sum(group_usage ** 2), 1e-10)  # Avoid division by zero\n",
        "                nc_values.append(nc if not np.isnan(nc) and np.isfinite(nc) else 0)\n",
        "            except:\n",
        "                nc_values.append(0)\n",
        "\n",
        "        stats['enc_approx'] = nc_values\n",
        "\n",
        "        # Codon Adaptation Index (CAI) - simple version\n",
        "        # Using high-expression genes pattern (simplified)\n",
        "        reference_pattern = df.mean()  # Use mean as reference\n",
        "        cai_values = []\n",
        "        for idx in df.index:\n",
        "            try:\n",
        "                aa_usage = df.loc[idx]\n",
        "                # Simplified CAI calculation with NaN handling\n",
        "                ratio = aa_usage / (reference_pattern + 1e-10)\n",
        "                ratio = ratio[~np.isnan(ratio)]  # Remove NaN values\n",
        "                if len(ratio) > 0:\n",
        "                    log_ratio = np.log(ratio + 1e-10)\n",
        "                    log_ratio = log_ratio[np.isfinite(log_ratio)]  # Remove inf values\n",
        "                    if len(log_ratio) > 0:\n",
        "                        cai = np.exp(np.mean(log_ratio))\n",
        "                        cai_values.append(cai if not np.isnan(cai) and np.isfinite(cai) else 1.0)\n",
        "                    else:\n",
        "                        cai_values.append(1.0)\n",
        "                else:\n",
        "                    cai_values.append(1.0)\n",
        "            except:\n",
        "                cai_values.append(1.0)\n",
        "\n",
        "        stats['cai_approx'] = cai_values\n",
        "\n",
        "    def add_cosine_similarity_feature(self):\n",
        "        \"\"\"Enhanced similarity features\"\"\"\n",
        "        df = self.df_aa_norm\n",
        "\n",
        "        # Similarity to mean profile\n",
        "        mean_profile = df.mean(axis=0).values.reshape(1, -1)\n",
        "        cos_sim_mean = cosine_similarity(df.values, mean_profile).flatten()\n",
        "\n",
        "        # Similarity to median profile\n",
        "        median_profile = df.median(axis=0).values.reshape(1, -1)\n",
        "        cos_sim_median = cosine_similarity(df.values, median_profile).flatten()\n",
        "\n",
        "        # Kingdom-specific similarities if available\n",
        "        kingdom_sims = []\n",
        "        if 'Kingdom' in self.codon_data.columns:\n",
        "            for kingdom in self.codon_data['Kingdom'].unique():\n",
        "                kingdom_mask = self.codon_data['Kingdom'] == kingdom\n",
        "                if kingdom_mask.sum() > 1:\n",
        "                    kingdom_profile = df[kingdom_mask].mean(axis=0).values.reshape(1, -1)\n",
        "                    kingdom_sim = cosine_similarity(df.values, kingdom_profile).flatten()\n",
        "                    kingdom_sims.append(pd.Series(kingdom_sim, index=df.index, name=f'cos_sim_{kingdom}'))\n",
        "\n",
        "        similarity_features = pd.DataFrame({\n",
        "            'cosine_similarity_to_mean': cos_sim_mean,\n",
        "            'cosine_similarity_to_median': cos_sim_median\n",
        "        }, index=df.index)\n",
        "\n",
        "        # Add kingdom similarities\n",
        "        for kingdom_sim in kingdom_sims:\n",
        "            similarity_features[kingdom_sim.name] = kingdom_sim\n",
        "\n",
        "        self.cos_sim = similarity_features\n",
        "        return self\n",
        "\n",
        "    def detect_and_handle_outliers(self, contamination=0.1):\n",
        "        \"\"\"Detect and optionally remove outliers\"\"\"\n",
        "        if self.features is None:\n",
        "            print(\"Features not yet combined. Skipping outlier detection.\")\n",
        "            return self\n",
        "\n",
        "        # Use Isolation Forest for outlier detection\n",
        "        iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
        "        outlier_labels = iso_forest.fit_predict(self.features.select_dtypes(include=[np.number]))\n",
        "\n",
        "        self.outliers_mask = outlier_labels == -1\n",
        "        n_outliers = self.outliers_mask.sum()\n",
        "\n",
        "        print(f\"Detected {n_outliers} outliers ({n_outliers/len(self.features)*100:.2f}%)\")\n",
        "\n",
        "        # Option to remove outliers\n",
        "        # self.features = self.features[~self.outliers_mask]\n",
        "        # self.codon_data = self.codon_data[~self.outliers_mask]\n",
        "\n",
        "        return self\n",
        "\n",
        "    def combine_features(self):\n",
        "        \"\"\"Enhanced feature combination with robust NaN handling\"\"\"\n",
        "        features = pd.concat([self.df_aa_norm, self.stats, self.cos_sim], axis=1)\n",
        "\n",
        "        # Encode categorical variables\n",
        "        if 'Kingdom' in self.codon_data.columns:\n",
        "            le = LabelEncoder()\n",
        "            kingdom_encoded = le.fit_transform(self.codon_data['Kingdom'])\n",
        "            features['kingdom_encoded'] = kingdom_encoded\n",
        "\n",
        "            # One-hot encode kingdom for better separation\n",
        "            kingdom_dummies = pd.get_dummies(self.codon_data['Kingdom'], prefix='kingdom')\n",
        "            features = pd.concat([features, kingdom_dummies], axis=1)\n",
        "\n",
        "        # Add interaction features (selected pairs) with NaN handling\n",
        "        if 'entropy' in features.columns and 'std' in features.columns:\n",
        "            features['entropy_std_interaction'] = (features['entropy'] * features['std']).fillna(0)\n",
        "\n",
        "        if 'mean' in features.columns and 'gini' in features.columns:\n",
        "            features['mean_gini_interaction'] = (features['mean'] * features['gini']).fillna(0)\n",
        "\n",
        "        # Handle all NaN values before variance filtering\n",
        "        features = features.fillna(0)\n",
        "\n",
        "        # Check for infinite values\n",
        "        features = features.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "        # Remove features with very low variance\n",
        "        numeric_features = features.select_dtypes(include=[np.number])\n",
        "        if not numeric_features.empty:\n",
        "            variance_selector = VarianceThreshold(threshold=1e-8)\n",
        "            try:\n",
        "                selected_mask = variance_selector.fit_transform(numeric_features)\n",
        "                selected_features = numeric_features.loc[:, variance_selector.get_support()]\n",
        "            except:\n",
        "                # If variance threshold fails, use all numeric features\n",
        "                selected_features = numeric_features\n",
        "\n",
        "            # Combine with non-numeric features if any\n",
        "            non_numeric = features.select_dtypes(exclude=[np.number])\n",
        "            if not non_numeric.empty:\n",
        "                features = pd.concat([selected_features, non_numeric], axis=1)\n",
        "            else:\n",
        "                features = selected_features\n",
        "\n",
        "        # Final check for NaN and infinite values\n",
        "        features = features.fillna(0)\n",
        "        features = features.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "        self.features = features\n",
        "        print(f\"Final feature shape: {features.shape}\")\n",
        "        print(f\"NaN count after processing: {features.isnull().sum().sum()}\")\n",
        "        return self\n",
        "\n",
        "    def advanced_scaling(self, method='robust'):\n",
        "        \"\"\"Use different scaling methods\"\"\"\n",
        "        if method == 'robust':\n",
        "            scaler = RobustScaler()\n",
        "        elif method == 'minmax':\n",
        "            scaler = MinMaxScaler()\n",
        "        else:\n",
        "            scaler = StandardScaler()\n",
        "\n",
        "        self.X_scaled = scaler.fit_transform(self.features_log)\n",
        "        print(f\"Applied {method} scaling\")\n",
        "        return self\n",
        "\n",
        "    def apply_pca(self, variance_threshold=0.85, max_components=None):\n",
        "        \"\"\"Enhanced PCA with better component selection\"\"\"\n",
        "        if max_components is None:\n",
        "            max_components = min(50, self.X_scaled.shape[1], self.X_scaled.shape[0] - 1)\n",
        "\n",
        "        pca_temp = PCA()\n",
        "        pca_temp.fit(self.X_scaled)\n",
        "        cumulative_variance = np.cumsum(pca_temp.explained_variance_ratio_)\n",
        "\n",
        "        # Find optimal number of components\n",
        "        n_components = np.argmax(cumulative_variance >= variance_threshold) + 1\n",
        "        n_components = min(n_components, max_components, self.X_scaled.shape[1])\n",
        "        n_components = max(1, n_components)\n",
        "\n",
        "        print(f\"PCA components to explain {variance_threshold*100}% variance: {n_components}\")\n",
        "        print(f\"Actual variance explained: {cumulative_variance[n_components-1]*100:.2f}%\")\n",
        "\n",
        "        pca = PCA(n_components=n_components)\n",
        "        self.X_pca = pca.fit_transform(self.X_scaled)\n",
        "        self.pca_model = pca\n",
        "\n",
        "        return self\n",
        "\n",
        "    def run_advanced_clustering(self, min_clusters=2, max_clusters=15, methods=['kmeans', 'gmm']):\n",
        "        \"\"\"Enhanced clustering with multiple algorithms\"\"\"\n",
        "        best_score = -1\n",
        "        best_labels = None\n",
        "        best_k = None\n",
        "        best_method = None\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for method in methods:\n",
        "            print(f\"\\n--- {method.upper()} Clustering ---\")\n",
        "            method_results = {}\n",
        "\n",
        "            for k in range(min_clusters, min(max_clusters + 1, self.X_pca.shape[0])):\n",
        "                try:\n",
        "                    if method == 'kmeans':\n",
        "                        model = KMeans(n_clusters=k, random_state=42, n_init=20, max_iter=500)\n",
        "                    elif method == 'gmm':\n",
        "                        model = GaussianMixture(n_components=k, random_state=42, max_iter=200)\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                    labels = model.fit_predict(self.X_pca)\n",
        "\n",
        "                    # Skip if all points are in one cluster\n",
        "                    if len(set(labels)) <= 1:\n",
        "                        continue\n",
        "\n",
        "                    score = silhouette_score(self.X_pca, labels)\n",
        "                    method_results[k] = {'score': score, 'labels': labels, 'model': model}\n",
        "\n",
        "                    print(f\"Clusters: {k}, Silhouette Score: {score:.4f}\")\n",
        "\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_labels = labels\n",
        "                        best_k = k\n",
        "                        best_method = method\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error with {method} k={k}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            results[method] = method_results\n",
        "\n",
        "        print(f\"\\n=== BEST RESULT ===\")\n",
        "        print(f\"Method: {best_method}, Clusters: {best_k}, Score: {best_score:.4f}\")\n",
        "\n",
        "        self.clustering_results = results\n",
        "        return best_labels, best_score, best_k, best_method\n",
        "\n",
        "    def optimize_clustering_parameters(self):\n",
        "        \"\"\"Fine-tune clustering parameters\"\"\"\n",
        "        print(\"Optimizing clustering parameters...\")\n",
        "\n",
        "        best_score = -1\n",
        "        best_params = None\n",
        "        best_labels = None\n",
        "\n",
        "        # Try different initialization methods and parameters for K-means\n",
        "        init_methods = ['k-means++', 'random']\n",
        "        n_init_values = [20, 50]\n",
        "        max_iter_values = [300, 500]\n",
        "\n",
        "        # Find rough optimal k first\n",
        "        silhouette_scores = []\n",
        "        k_range = range(2, min(16, self.X_pca.shape[0]))\n",
        "\n",
        "        for k in k_range:\n",
        "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "            labels = kmeans.fit_predict(self.X_pca)\n",
        "            if len(set(labels)) > 1:\n",
        "                score = silhouette_score(self.X_pca, labels)\n",
        "                silhouette_scores.append(score)\n",
        "            else:\n",
        "                silhouette_scores.append(-1)\n",
        "\n",
        "        # Focus on top k values\n",
        "        top_k_indices = np.argsort(silhouette_scores)[-3:]  # Top 3\n",
        "        top_k_values = [k_range[i] for i in top_k_indices if silhouette_scores[i] > 0]\n",
        "\n",
        "        print(f\"Top K values to optimize: {top_k_values}\")\n",
        "\n",
        "        # Fine-tune parameters for top k values\n",
        "        for k in top_k_values:\n",
        "            for init_method in init_methods:\n",
        "                for n_init in n_init_values:\n",
        "                    for max_iter in max_iter_values:\n",
        "                        try:\n",
        "                            kmeans = KMeans(\n",
        "                                n_clusters=k,\n",
        "                                init=init_method,\n",
        "                                n_init=n_init,\n",
        "                                max_iter=max_iter,\n",
        "                                random_state=42\n",
        "                            )\n",
        "                            labels = kmeans.fit_predict(self.X_pca)\n",
        "\n",
        "                            if len(set(labels)) > 1:\n",
        "                                score = silhouette_score(self.X_pca, labels)\n",
        "\n",
        "                                if score > best_score:\n",
        "                                    best_score = score\n",
        "                                    best_labels = labels\n",
        "                                    best_params = {\n",
        "                                        'n_clusters': k,\n",
        "                                        'init': init_method,\n",
        "                                        'n_init': n_init,\n",
        "                                        'max_iter': max_iter,\n",
        "                                        'score': score\n",
        "                                    }\n",
        "                        except:\n",
        "                            continue\n",
        "\n",
        "        print(f\"Best optimized parameters: {best_params}\")\n",
        "        return best_labels, best_score, best_params\n",
        "\n",
        "    def full_pipeline(self,\n",
        "                      fill_na=True,\n",
        "                      remove_outliers=False,\n",
        "                      do_aggregate=True,\n",
        "                      do_normalize=True,\n",
        "                      add_advanced_stats=True,\n",
        "                      add_cosine=True,\n",
        "                      detect_outliers=True,\n",
        "                      do_log=True,\n",
        "                      scaling_method='robust',\n",
        "                      do_pca=True,\n",
        "                      variance_threshold=0.85,\n",
        "                      optimize_params=True):\n",
        "        \"\"\"Enhanced full pipeline with robust error handling\"\"\"\n",
        "\n",
        "        print(\"=== ENHANCED CODON CLUSTERING PIPELINE ===\")\n",
        "\n",
        "        # Data cleaning\n",
        "        self.clean_data(fill_na=fill_na, remove_outliers=remove_outliers)\n",
        "\n",
        "        if do_aggregate:\n",
        "            self.aggregate_codon_to_aa()\n",
        "\n",
        "        if do_normalize:\n",
        "            self.normalize_per_row()\n",
        "\n",
        "        if add_advanced_stats:\n",
        "            self.add_advanced_features()\n",
        "        else:\n",
        "            self.add_global_statistics()  # Fallback to basic stats\n",
        "\n",
        "        if add_cosine:\n",
        "            self.add_cosine_similarity_feature()\n",
        "\n",
        "        self.combine_features()\n",
        "\n",
        "        if detect_outliers:\n",
        "            self.detect_and_handle_outliers()\n",
        "\n",
        "        # Log transform with NaN handling\n",
        "        if do_log:\n",
        "            numeric_features = self.features.select_dtypes(include=[np.number])\n",
        "            # Handle zeros and negative values before log transform\n",
        "            numeric_features = numeric_features.clip(lower=0)  # Ensure no negative values\n",
        "            self.features_log = np.log1p(numeric_features)\n",
        "        else:\n",
        "            self.features_log = self.features.select_dtypes(include=[np.number]).copy()\n",
        "\n",
        "        # Final NaN check before scaling\n",
        "        self.features_log = self.features_log.fillna(0)\n",
        "        self.features_log = self.features_log.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "        print(f\"Features shape before scaling: {self.features_log.shape}\")\n",
        "        print(f\"NaN count before scaling: {self.features_log.isnull().sum().sum()}\")\n",
        "        print(f\"Inf count before scaling: {np.isinf(self.features_log.values).sum()}\")\n",
        "\n",
        "        self.advanced_scaling(method=scaling_method)\n",
        "\n",
        "        if do_pca and self.X_scaled.shape[0] > 1 and self.X_scaled.shape[1] > 0:\n",
        "            self.apply_pca(variance_threshold=variance_threshold)\n",
        "        else:\n",
        "            self.X_pca = self.X_scaled\n",
        "\n",
        "        print(f\"\\nFinal data shape for clustering: {self.X_pca.shape}\")\n",
        "\n",
        "        # Enhanced clustering\n",
        "        if optimize_params:\n",
        "            return self.optimize_clustering_parameters()\n",
        "        else:\n",
        "            return self.run_advanced_clustering()\n",
        "\n",
        "    def visualize_clusters(self, method='tsne', labels=None, use_pca=True, title=None, figsize=(12, 8)):\n",
        "        \"\"\"Enhanced visualization\"\"\"\n",
        "        X_input = self.X_pca if use_pca and self.X_pca is not None else self.X_scaled\n",
        "\n",
        "        if X_input is None or X_input.shape[0] < 2:\n",
        "            print(f\"Skipping visualization ({method}): Insufficient data.\")\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=figsize)\n",
        "\n",
        "        if method == 'tsne':\n",
        "            perplexity = min(30, max(2, X_input.shape[0] - 1))\n",
        "            tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, max_iter=1000)\n",
        "            X_vis = tsne.fit_transform(X_input)\n",
        "            title = title or \"t-SNE Visualization\"\n",
        "        elif method == 'umap':\n",
        "            n_neighbors = min(15, max(2, X_input.shape[0] - 1))\n",
        "            reducer = umap.UMAP(n_neighbors=n_neighbors, random_state=42, min_dist=0.1)\n",
        "            X_vis = reducer.fit_transform(X_input)\n",
        "            title = title or \"UMAP Visualization\"\n",
        "        else:\n",
        "            raise ValueError(\"method must be 'tsne' or 'umap'\")\n",
        "\n",
        "        if labels is not None:\n",
        "            scatter = plt.scatter(X_vis[:, 0], X_vis[:, 1], c=labels, cmap='tab10', s=60, alpha=0.7)\n",
        "            plt.colorbar(scatter, label=\"Cluster Labels\")\n",
        "\n",
        "            # Add cluster centers\n",
        "            unique_labels = np.unique(labels)\n",
        "            for label in unique_labels:\n",
        "                if label == -1:  # Skip noise points in DBSCAN\n",
        "                    continue\n",
        "                mask = labels == label\n",
        "                center_x = np.mean(X_vis[mask, 0])\n",
        "                center_y = np.mean(X_vis[mask, 1])\n",
        "                plt.scatter(center_x, center_y, c='red', marker='x', s=200, linewidths=3)\n",
        "        else:\n",
        "            plt.scatter(X_vis[:, 0], X_vis[:, 1], s=60, alpha=0.7)\n",
        "\n",
        "        plt.title(title, fontsize=14)\n",
        "        plt.xlabel(f'{method.upper()} 1', fontsize=12)\n",
        "        plt.ylabel(f'{method.upper()} 2', fontsize=12)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def analyze_clusters(self, labels, method_name=\"Clustering\"):\n",
        "        \"\"\"Analyze cluster characteristics\"\"\"\n",
        "        if labels is None:\n",
        "            return\n",
        "\n",
        "        print(f\"\\n=== {method_name} Analysis ===\")\n",
        "        unique_labels = np.unique(labels)\n",
        "        print(f\"Number of clusters: {len(unique_labels)}\")\n",
        "\n",
        "        for label in unique_labels:\n",
        "            mask = labels == label\n",
        "            count = mask.sum()\n",
        "            percentage = count / len(labels) * 100\n",
        "            print(f\"Cluster {label}: {count} samples ({percentage:.1f}%)\")\n",
        "\n",
        "        # Analyze by Kingdom if available\n",
        "        if 'Kingdom' in self.codon_data.columns:\n",
        "            print(\"\\nCluster composition by Kingdom:\")\n",
        "            df_analysis = pd.DataFrame({\n",
        "                'Kingdom': self.codon_data['Kingdom'].values,\n",
        "                'Cluster': labels\n",
        "            })\n",
        "\n",
        "            cluster_kingdom = pd.crosstab(df_analysis['Cluster'], df_analysis['Kingdom'], normalize='index') * 100\n",
        "            print(cluster_kingdom.round(1))\n",
        "\n",
        "\n",
        "# Usage example:\n",
        "# pipeline = ImprovedCodonClusteringPipeline(codon_data)\n",
        "# best_labels, best_score, best_params = pipeline.full_pipeline(optimize_params=True)\n",
        "# pipeline.visualize_clusters(labels=best_labels)\n",
        "# pipeline.analyze_clusters(best_labels, \"Optimized K-means\")"
      ],
      "metadata": {
        "id": "nXdjQPJbZyax"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CodonClassificationPipeline:\n",
        "    def __init__(self, clustered_data, pipeline_object):\n",
        "        \"\"\"\n",
        "        clustered_data: DataFrame dengan kolom cluster hasil dari clustering\n",
        "        pipeline_object: Object ImprovedCodonClusteringPipeline yang sudah dijalankan\n",
        "        \"\"\"\n",
        "        self.data = clustered_data.copy()\n",
        "        self.pipeline_obj = pipeline_object\n",
        "        self.codon_cols = [col for col in self.data.columns if col not in ['Species', 'Kingdom', 'Phylum', 'Class', 'Order', 'Cluster']]\n",
        "        self.X = None\n",
        "        self.y = None\n",
        "        self.models = {}\n",
        "        self.feature_importance = {}\n",
        "        self.classification_results = {}\n",
        "\n",
        "    def prepare_classification_data(self):\n",
        "        print(\"=== PREPARING CLASSIFICATION DATA ===\")\n",
        "        if hasattr(self.pipeline_obj, 'features_log') and self.pipeline_obj.features_log is not None:\n",
        "            self.X = self.pipeline_obj.features_log.reindex(self.data.index).dropna()\n",
        "        else:\n",
        "            self.X = self.data[self.codon_cols].copy().dropna()\n",
        "        self.y = self.data.loc[self.X.index, 'Cluster'].copy()\n",
        "        print(f\"Feature shape: {self.X.shape}\")\n",
        "        print(f\"Target distribution:\")\n",
        "        print(self.y.value_counts().sort_index())\n",
        "        if not self.X.index.equals(self.y.index):\n",
        "             raise IndexError(\"Index mismatch between features (X) and labels (y) after preparation!\")\n",
        "        return self\n",
        "\n",
        "    def run_multiple_classifiers(self, test_size=0.2, random_state=42):\n",
        "        print(\"\\n=== RUNNING MULTIPLE CLASSIFIERS ===\")\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            self.X, self.y, test_size=test_size, random_state=random_state, stratify=self.y\n",
        "        )\n",
        "        classifiers = {\n",
        "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=random_state),\n",
        "            'XGBoost': xgb.XGBClassifier(random_state=random_state, eval_metric='mlogloss'),\n",
        "            'SVM': SVC(random_state=random_state, probability=True),\n",
        "            'Logistic Regression': LogisticRegression(random_state=random_state, max_iter=1000)\n",
        "        }\n",
        "        results = {}\n",
        "        for name, clf in classifiers.items():\n",
        "            print(f\"\\nTraining {name}...\")\n",
        "            try:\n",
        "                clf.fit(X_train, y_train)\n",
        "                y_pred = clf.predict(X_test)\n",
        "                y_pred_proba = clf.predict_proba(X_test) if hasattr(clf, 'predict_proba') else None\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                cv_scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
        "                results[name] = {\n",
        "                    'model': clf,\n",
        "                    'accuracy': accuracy,\n",
        "                    'cv_mean': cv_scores.mean(),\n",
        "                    'cv_std': cv_scores.std(),\n",
        "                    'y_test': y_test,\n",
        "                    'y_pred': y_pred,\n",
        "                    'y_pred_proba': y_pred_proba\n",
        "                }\n",
        "                print(f\"{name} - Accuracy: {accuracy:.4f}, CV: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {name}: {e}\")\n",
        "                continue\n",
        "        self.classification_results = results\n",
        "        return self\n",
        "\n",
        "    def optimize_best_classifier(self, n_iter=20, cv=3):\n",
        "        print(\"\\n=== OPTIMIZING BEST CLASSIFIER ===\")\n",
        "        if not self.classification_results:\n",
        "            print(\"No classification results found. Run run_multiple_classifiers() first.\")\n",
        "            return self\n",
        "        best_name = max(self.classification_results.keys(),\n",
        "                      key=lambda x: self.classification_results[x]['cv_mean'])\n",
        "        print(f\"Best classifier: {best_name}\")\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            self.X, self.y, test_size=0.2, random_state=42, stratify=self.y\n",
        "        )\n",
        "        param_grids = {\n",
        "            'Random Forest': {\n",
        "                'n_estimators': [100, 200],\n",
        "                'max_depth': [10, None],\n",
        "                'min_samples_split': [2, 5],\n",
        "                'min_samples_leaf': [1, 2]\n",
        "            },\n",
        "            'XGBoost': {\n",
        "                'n_estimators': [100, 200],\n",
        "                'max_depth': [3, 6],\n",
        "                'learning_rate': [0.01, 0.1],\n",
        "                'subsample': [0.8, 1.0]\n",
        "            },\n",
        "            'SVM': {\n",
        "                'C': [0.1, 1, 10],\n",
        "                'gamma': ['scale'],\n",
        "                'kernel': ['rbf']\n",
        "            },\n",
        "            'Logistic Regression': {\n",
        "                'C': [0.1, 1, 10],\n",
        "                'penalty': ['l2'],\n",
        "                'solver': ['liblinear']\n",
        "            }\n",
        "        }\n",
        "        if best_name in param_grids:\n",
        "            print(f\"Optimizing {best_name} with {n_iter} randomized search iterations...\")\n",
        "            if best_name == 'Random Forest':\n",
        "                base_model = RandomForestClassifier(random_state=42)\n",
        "            elif best_name == 'XGBoost':\n",
        "                base_model = xgb.XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
        "            elif best_name == 'SVM':\n",
        "                base_model = SVC(random_state=42, probability=True)\n",
        "            else:\n",
        "                base_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "            random_search = RandomizedSearchCV(\n",
        "                base_model,\n",
        "                param_distributions=param_grids[best_name],\n",
        "                n_iter=n_iter,\n",
        "                cv=cv,\n",
        "                scoring='accuracy',\n",
        "                n_jobs=-1,\n",
        "                random_state=42\n",
        "            )\n",
        "            random_search.fit(X_train, y_train)\n",
        "            best_model = random_search.best_estimator_\n",
        "            y_pred = best_model.predict(X_test)\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            print(f\"Best parameters: {random_search.best_params_}\")\n",
        "            print(f\"Optimized accuracy: {accuracy:.4f}\")\n",
        "            self.classification_results[f'{best_name}_Optimized'] = {\n",
        "                'model': best_model,\n",
        "                'accuracy': accuracy,\n",
        "                'y_test': y_test,\n",
        "                'y_pred': y_pred,\n",
        "                'best_params': random_search.best_params_\n",
        "            }\n",
        "        return self\n",
        "\n",
        "    def extract_feature_importance(self, top_n=20):\n",
        "        print(f\"\\n=== EXTRACTING TOP {top_n} FEATURE IMPORTANCE ===\")\n",
        "        feature_names = self.X.columns\n",
        "        for name, result in self.classification_results.items():\n",
        "            model = result['model']\n",
        "            if hasattr(model, 'feature_importances_'):\n",
        "                importance = model.feature_importances_\n",
        "                importance_df = pd.DataFrame({\n",
        "                    'Feature': feature_names,\n",
        "                    'Importance': importance\n",
        "                }).sort_values('Importance', ascending=False)\n",
        "                self.feature_importance[name] = importance_df\n",
        "                print(f\"\\n{name} - Top {min(top_n, len(importance_df))} Important Features:\")\n",
        "                print(importance_df.head(top_n))\n",
        "            elif hasattr(model, 'coef_'):\n",
        "                if len(model.coef_.shape) > 1:\n",
        "                    importance = np.mean(np.abs(model.coef_), axis=0)\n",
        "                else:\n",
        "                    importance = np.abs(model.coef_)\n",
        "                importance_df = pd.DataFrame({\n",
        "                    'Feature': feature_names,\n",
        "                    'Importance': importance\n",
        "                }).sort_values('Importance', ascending=False)\n",
        "                self.feature_importance[name] = importance_df\n",
        "                print(f\"\\n{name} - Top {min(top_n, len(importance_df))} Important Features:\")\n",
        "                print(importance_df.head(top_n))\n",
        "        return self\n",
        "\n",
        "    def analyze_cluster_characteristics(self):\n",
        "        print(\"\\n=== ANALYZING CLUSTER CHARACTERISTICS ===\")\n",
        "        cluster_analysis = {}\n",
        "        for cluster in sorted(self.y.unique()):\n",
        "            cluster_mask = self.y == cluster\n",
        "            cluster_data = self.X[cluster_mask]\n",
        "            cluster_stats = {\n",
        "                'mean': cluster_data.mean(),\n",
        "                'std': cluster_data.std(),\n",
        "                'size': cluster_mask.sum()\n",
        "            }\n",
        "            top_features = cluster_stats['mean'].sort_values(ascending=False).head(10)\n",
        "            cluster_analysis[cluster] = {\n",
        "                'stats': cluster_stats,\n",
        "                'top_features': top_features,\n",
        "                'size': cluster_stats['size']\n",
        "            }\n",
        "            print(f\"\\nCluster {cluster} ({cluster_stats['size']} samples):\")\n",
        "            print(\"Top features (highest mean):\")\n",
        "            print(top_features)\n",
        "        self.cluster_analysis = cluster_analysis\n",
        "        return self\n",
        "\n",
        "    def plot_feature_importance(self, model_name=None, top_n=15, figsize=(10, 8)):\n",
        "        if not self.feature_importance:\n",
        "            print(\"No feature importance data. Run extract_feature_importance() first.\")\n",
        "            return\n",
        "        if model_name is None:\n",
        "            model_name = list(self.feature_importance.keys())[0]\n",
        "        if model_name not in self.feature_importance:\n",
        "            print(f\"Model {model_name} not found in feature importance data.\")\n",
        "            return\n",
        "        importance_df = self.feature_importance[model_name].head(top_n)\n",
        "        plt.figure(figsize=figsize)\n",
        "        sns.barplot(data=importance_df, y='Feature', x='Importance', palette='viridis')\n",
        "        plt.title(f'Top {top_n} Feature Importance - {model_name}', fontsize=14)\n",
        "        plt.xlabel('Importance Score', fontsize=12)\n",
        "        plt.ylabel('Features', fontsize=12)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_cluster_heatmap(self, top_n=15, figsize=(12, 8)):\n",
        "        if not hasattr(self, 'cluster_analysis'):\n",
        "            self.analyze_cluster_characteristics()\n",
        "        all_top_features = set()\n",
        "        for cluster_data in self.cluster_analysis.values():\n",
        "            all_top_features.update(cluster_data['top_features'].head(top_n).index)\n",
        "        heatmap_data = []\n",
        "        cluster_labels = []\n",
        "        for cluster in sorted(self.cluster_analysis.keys()):\n",
        "            cluster_means = self.cluster_analysis[cluster]['stats']['mean']\n",
        "            cluster_row = [cluster_means.get(feature, 0) for feature in all_top_features]\n",
        "            heatmap_data.append(cluster_row)\n",
        "            cluster_labels.append(f'Cluster {cluster}')\n",
        "        heatmap_df = pd.DataFrame(heatmap_data,\n",
        "                                columns=list(all_top_features),\n",
        "                                index=cluster_labels)\n",
        "        plt.figure(figsize=figsize)\n",
        "        sns.heatmap(heatmap_df, annot=True, cmap='viridis', fmt='.3f')\n",
        "        plt.title('Cluster Characteristics Heatmap', fontsize=14)\n",
        "        plt.xlabel('Features', fontsize=12)\n",
        "        plt.ylabel('Clusters', fontsize=12)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_confusion_matrices(self, figsize=(15, 4)):\n",
        "        if not self.classification_results:\n",
        "            print(\"No classification results found.\")\n",
        "            return\n",
        "        num_models = len(self.classification_results)\n",
        "        fig, axes = plt.subplots(1, num_models, figsize=figsize, squeeze=False)\n",
        "        for i, (name, result) in enumerate(self.classification_results.items()):\n",
        "            y_test = result['y_test']\n",
        "            y_pred = result['y_pred']\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, i])\n",
        "            axes[0, i].set_title(f'{name} Confusion Matrix', fontsize=12)\n",
        "            axes[0, i].set_xlabel('Predicted Label', fontsize=10)\n",
        "            axes[0, i].set_ylabel('True Label', fontsize=10)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def evaluate_best_model(self):\n",
        "        if not self.classification_results:\n",
        "            print(\"No classification results found. Run run_multiple_classifiers() first.\")\n",
        "            return None\n",
        "        best_name = None\n",
        "        best_acc = -1\n",
        "        optimized_results = {k: v for k, v in self.classification_results.items() if '_Optimized' in k}\n",
        "        if optimized_results:\n",
        "            best_name = max(optimized_results.keys(), key=lambda x: optimized_results[x].get('accuracy', -1))\n",
        "            best_acc = optimized_results[best_name].get('accuracy', -1)\n",
        "            print(f\"Evaluating optimized model: {best_name}\")\n",
        "        else:\n",
        "            base_results = {k: v for k, v in self.classification_results.items() if '_Optimized' not in k}\n",
        "            if base_results:\n",
        "                best_name = max(base_results.keys(), key=lambda x: base_results[x].get('cv_mean', -1))\n",
        "                best_acc = base_results[best_name].get('accuracy', -1)\n",
        "                print(f\"Evaluating base model with best CV score: {best_name}\")\n",
        "        if best_name is None or best_acc == -1:\n",
        "            print(\"Could not identify a suitable model for evaluation.\")\n",
        "            return None\n",
        "        best_result = self.classification_results[best_name]\n",
        "        y_test = best_result['y_test']\n",
        "        y_pred = best_result['y_pred']\n",
        "        y_pred_proba = best_result.get('y_pred_proba')\n",
        "        print(f\"\\n--- Evaluation Metrics for {best_name} ---\")\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        print(f\"Accuracy           : {accuracy:.4f}\")\n",
        "        print(f\"Precision (weighted): {precision:.4f}\")\n",
        "        print(f\"Recall (weighted)   : {recall:.4f}\")\n",
        "        print(f\"F1 Score (weighted) : {f1:.4f}\")\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(cm)\n",
        "        if y_pred_proba is not None:\n",
        "            try:\n",
        "                unique_labels = np.unique(y_test)\n",
        "                if len(unique_labels) == 2:\n",
        "                    roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
        "                    print(f\"ROC AUC            : {roc_auc:.4f}\")\n",
        "                elif len(unique_labels) > 2:\n",
        "                    from sklearn.preprocessing import label_binarize\n",
        "                    classes = unique_labels\n",
        "                    y_test_bin = label_binarize(y_test, classes=classes)\n",
        "                    roc_auc_ovr = roc_auc_score(y_test_bin, y_pred_proba, average='macro')\n",
        "                    print(f\"ROC AUC OvR (macro): {roc_auc_ovr:.4f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Could not calculate ROC AUC: {e}\")\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred, zero_division=0))\n",
        "        metrics = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision_weighted': precision,\n",
        "            'recall_weighted': recall,\n",
        "            'f1_weighted': f1,\n",
        "            'confusion_matrix': cm\n",
        "        }\n",
        "        if y_pred_proba is not None:\n",
        "            metrics['roc_auc'] = roc_auc if 'roc_auc' in locals() else None\n",
        "        return metrics\n",
        "\n",
        "    def full_classification_pipeline(self, test_size=0.2, random_state=42, optimize=True, n_iter_optimize=20, cv_optimize=3, top_features=20):\n",
        "        print(\"\\n=== STARTING FULL CLASSIFICATION PIPELINE ===\")\n",
        "        self.prepare_classification_data()\n",
        "        self.run_multiple_classifiers(test_size=test_size, random_state=random_state)\n",
        "        if optimize:\n",
        "            self.optimize_best_classifier(n_iter=n_iter_optimize, cv=cv_optimize)\n",
        "        self.extract_feature_importance(top_n=top_features)\n",
        "        self.analyze_cluster_characteristics()\n",
        "        self.evaluate_best_model()\n",
        "        self.plot_confusion_matrices()\n",
        "        self.save_classification_results()\n",
        "        print(\"\\n=== FULL CLASSIFICATION PIPELINE COMPLETED ===\")\n",
        "\n",
        "    def save_classification_results(self, output_dir=\"classification_results\"):\n",
        "        print(f\"\\n=== SAVING CLASSIFICATION RESULTS to {output_dir} ===\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        for name, importance_df in self.feature_importance.items():\n",
        "            filename = f\"{output_dir}/{name.lower().replace(' ', '_')}_feature_importance.csv\"\n",
        "            importance_df.to_csv(filename, index=False)\n",
        "            print(f\"Saved {filename}\")\n",
        "        if hasattr(self, 'cluster_analysis'):\n",
        "            analysis_list = []\n",
        "            for cluster, data in self.cluster_analysis.items():\n",
        "                row = {'Cluster': cluster, 'Size': data['size']}\n",
        "                top_features_str = ', '.join([f\"{feat}:{val:.4f}\" for feat, val in data['top_features'].items()])\n",
        "                row['Top_Features_Mean'] = top_features_str\n",
        "                analysis_list.append(row)\n",
        "            analysis_df = pd.DataFrame(analysis_list)\n",
        "            filename = f\"{output_dir}/cluster_characteristics_analysis.csv\"\n",
        "            analysis_df.to_csv(filename, index=False)\n",
        "            print(f\"Saved {filename}\")\n",
        "        if self.classification_results:\n",
        "            comparison_list = []\n",
        "            for name, result in self.classification_results.items():\n",
        "                row = {\n",
        "                    'Model': name,\n",
        "                    'Accuracy': result.get('accuracy'),\n",
        "                    'CV_Mean_Accuracy': result.get('cv_mean'),\n",
        "                    'CV_Std_Accuracy': result.get('cv_std'),\n",
        "                    'Parameters': result.get('best_params', 'N/A')\n",
        "                }\n",
        "                comparison_list.append(row)\n",
        "            comparison_df = pd.DataFrame(comparison_list)\n",
        "            filename = f\"{output_dir}/model_comparison_summary.csv\"\n",
        "            comparison_df.to_csv(filename, index=False)\n",
        "            print(f\"Saved {filename}\")\n",
        "        print(\"Saving complete.\")\n"
      ],
      "metadata": {
        "id": "Wsn5Ggo3Dt4k"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_cluster_results(df, labels, filename, cluster_col_name='Cluster'):\n",
        "    \"\"\"\n",
        "    Simpan dataset awal dengan kolom hasil clustering ke file CSV.\n",
        "    df: DataFrame asli\n",
        "    labels: hasil cluster (numpy array atau pd.Series)\n",
        "    filename: nama file output CSV\n",
        "    cluster_col_name: nama kolom cluster baru\n",
        "    \"\"\"\n",
        "    if len(labels) != len(df):\n",
        "        if isinstance(labels, pd.Series) and labels.index.equals(df.index):\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\"Panjang labels tidak sesuai dengan jumlah baris dataframe.\")\n",
        "\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    if isinstance(labels, np.ndarray) and len(labels) == len(df_copy):\n",
        "        df_copy[cluster_col_name] = labels\n",
        "    elif isinstance(labels, pd.Series):\n",
        "        labels_aligned = labels.reindex(df_copy.index)\n",
        "        if labels_aligned.isnull().any():\n",
        "            raise ValueError(\"Labels berisi indeks yang tidak ditemukan di dataframe.\")\n",
        "        df_copy[cluster_col_name] = labels_aligned.values\n",
        "    else:\n",
        "        raise TypeError(\"Labels harus berupa numpy array atau pandas Series.\")\n",
        "\n",
        "    df_copy.to_csv(filename, index=False)\n",
        "    print(f\"Saved clustering results to {filename}\")\n"
      ],
      "metadata": {
        "id": "cm_3hbI-MCVg"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_clustering(features, labels):\n",
        "    print(\"=== CLUSTERING EVALUATION METRICS ===\")\n",
        "\n",
        "    # Check if features and labels are aligned\n",
        "    assert len(features) == len(labels), \"Features and labels length mismatch!\"\n",
        "\n",
        "    # Silhouette Score\n",
        "    sil_score = silhouette_score(features, labels)\n",
        "    print(f\"Silhouette Score       : {sil_score:.4f}\")\n",
        "\n",
        "    # Davies-Bouldin Index\n",
        "    db_index = davies_bouldin_score(features, labels)\n",
        "    print(f\"Davies-Bouldin Index   : {db_index:.4f}\")\n",
        "\n",
        "    # Calinski-Harabasz Index\n",
        "    ch_index = calinski_harabasz_score(features, labels)\n",
        "    print(f\"Calinski-Harabasz Index: {ch_index:.4f}\")\n",
        "\n",
        "    # Cluster size distribution\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    cluster_sizes = pd.Series(counts, index=unique)\n",
        "    print(\"Cluster size distribution:\")\n",
        "    print(cluster_sizes.sort_index())\n",
        "\n",
        "    return {\n",
        "        'silhouette_score': sil_score,\n",
        "        'davies_bouldin_index': db_index,\n",
        "        'calinski_harabasz_index': ch_index,\n",
        "        'cluster_sizes': cluster_sizes.to_dict()\n",
        "    }"
      ],
      "metadata": {
        "id": "vacBJv8bHPSS"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USAGE EXAMPLE - COMPLETE PIPELINE\n",
        "# ===============================\n",
        "from joblib import dump\n",
        "# Karena dataset codon_data sudah ada, langsung jalankan pipeline lengkap:\n",
        "\n",
        "# Step 1: Initialize clustering pipeline\n",
        "print(\"=== STEP 1: CLUSTERING ===\")\n",
        "pipeline = ImprovedCodonClusteringPipeline(codon_data)\n",
        "best_labels, best_score, best_params = pipeline.full_pipeline(optimize_params=True)\n",
        "features_log = pipeline.features_log\n",
        "dump(features_log, \"features_log.pkl\")\n",
        "print(\"features_log.pkl saved!\")\n",
        "\n",
        "# Step 2: Visualize clustering results\n",
        "print(\"\\n=== STEP 2: CLUSTERING VISUALIZATION ===\")\n",
        "pipeline.visualize_clusters(labels=best_labels, method='tsne')\n",
        "pipeline.visualize_clusters(labels=best_labels, method='umap')\n",
        "pipeline.analyze_clusters(best_labels, \"Optimized K-means\")\n",
        "\n",
        "# Step 3: Save clustering results\n",
        "print(\"\\n=== STEP 3: SAVING CLUSTERING RESULTS ===\")\n",
        "save_cluster_results(pipeline.codon_data, best_labels, \"hasil_clustering.csv\", cluster_col_name=\"Cluster\")\n",
        "\n",
        "# Step 4: Load results for classification\n",
        "print(\"\\n=== STEP 4: PREPARING FOR CLASSIFICATION ===\")\n",
        "results = pd.read_csv(\"hasil_clustering.csv\")\n",
        "print(f\"Loaded clustered data: {results.shape}\")\n",
        "print(f\"Cluster distribution:\")\n",
        "print(results['Cluster'].value_counts().sort_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB0t8hnnEbFx",
        "outputId": "b2e7f91e-e0db-421b-a0ec-20710ed5836e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== STEP 1: CLUSTERING ===\n",
            "=== ENHANCED CODON CLUSTERING PIPELINE ===\n",
            "Duplicate Rows: 0\n",
            "Data shape after cleaning: (13028, 69)\n",
            "Data shape after filtering low-count rows: (12375, 20)\n",
            "Final feature shape: (12375, 65)\n",
            "NaN count after processing: 0\n",
            "Detected 1238 outliers (10.00%)\n",
            "Features shape before scaling: (12375, 54)\n",
            "NaN count before scaling: 0\n",
            "Inf count before scaling: 0\n",
            "Applied robust scaling\n",
            "PCA components to explain 85.0% variance: 8\n",
            "Actual variance explained: 86.79%\n",
            "\n",
            "Final data shape for clustering: (12375, 8)\n",
            "Optimizing clustering parameters...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example after your pipeline run:\n",
        "metrics = evaluate_clustering(pipeline.X_pca, best_labels)"
      ],
      "metadata": {
        "id": "C26QBDmvHTPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "ll1xRkKGG-Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = CodonClassificationPipeline(results, pipeline)\n",
        "classifier.full_classification_pipeline(test_size=0.2, optimize=True, top_features=20)\n",
        "\n",
        "print(\"\\n=== PIPELINE COMPLETED SUCCESSFULLY ===\")\n",
        "print(\"Generated files:\")\n",
        "print(\"- hasil_clustering.csv (species with cluster labels)\")\n",
        "print(\"- classification_results_*_feature_importance.csv\")\n",
        "print(\"- classification_results_cluster_analysis.csv\")\n",
        "print(\"- classification_results_model_comparison.csv\")\n",
        "print(\"\\nAll visualizations have been displayed above!\")"
      ],
      "metadata": {
        "id": "eP4mHhARdbeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Rename files to simplify download (no spaces)\n",
        "os.rename('classification_results_Random Forest_feature_importance.csv', 'rf_importance.csv')\n",
        "os.rename('classification_results_XGBoost_feature_importance.csv', 'xgb_importance.csv')\n",
        "os.rename('classification_results_Logistic Regression_feature_importance.csv', 'lr_importance.csv')\n",
        "os.rename('classification_results_Random Forest_Optimized_feature_importance.csv', 'rf_optimized_importance.csv')\n",
        "os.rename('classification_results_cluster_analysis.csv', 'cluster_analysis.csv')\n",
        "os.rename('classification_results_model_comparison.csv', 'model_comparison.csv')\n",
        "os.rename('hasil_clustering.csv', 'hasil_clustering_clean.csv')"
      ],
      "metadata": {
        "id": "ccP88G71jShm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load cluster analysis\n",
        "cluster_df = pd.read_csv(\"cluster_analysis.csv\")\n",
        "\n",
        "# Load feature importance (misal Random Forest)\n",
        "importance_df = pd.read_csv(\"rf_importance.csv\")"
      ],
      "metadata": {
        "id": "ABlKuCKTmrwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_df"
      ],
      "metadata": {
        "id": "x99exa1Gzf8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df"
      ],
      "metadata": {
        "id": "xiYLVDZBzh4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping dictionary\n",
        "feature_description_full = {\n",
        "    'kingdom_encoded': {\n",
        "        'description': 'Kingdom Taxonomy Label',\n",
        "        'detail': 'Encoded label representing the taxonomic kingdom (e.g., Bacteria, Archaea, Eukaryota). Often used as a categorical identifier in classification tasks.'\n",
        "    },\n",
        "    'charged_negative_usage': {\n",
        "        'description': 'Usage of Negatively Charged Amino Acids',\n",
        "        'detail': 'Indicates the codon usage frequency for negatively charged amino acids like Aspartic acid (D) and Glutamic acid (E). High values may suggest adaptation for acidic protein functions.'\n",
        "    },\n",
        "    'charged_positive_usage': {\n",
        "        'description': 'Usage of Positively Charged Amino Acids',\n",
        "        'detail': 'Reflects the preference in using codons for basic amino acids such as Lysine (K), Arginine (R), and Histidine (H). Relevant in proteins involved in nucleic acid interactions.'\n",
        "    },\n",
        "    'hydrophobic_usage': {\n",
        "        'description': 'Hydrophobic Amino Acid Usage',\n",
        "        'detail': 'Frequency of codons coding for hydrophobic amino acids, often found in membrane-spanning regions or protein cores.'\n",
        "    },\n",
        "    'entropy': {\n",
        "        'description': 'Codon Usage Entropy (Diversity)',\n",
        "        'detail': 'Measures how evenly codons are used across synonymous options. Lower entropy indicates stronger bias, which may be linked to high expression efficiency.'\n",
        "    },\n",
        "    'mean_gini_interaction': {\n",
        "        'description': 'Mean Gini Codon Interaction',\n",
        "        'detail': 'Represents average inequality in codon usage interactions across features. Higher values imply more bias or selection in codon usage.'\n",
        "    },\n",
        "    'std': {\n",
        "        'description': 'Standard Deviation of Codon Usage',\n",
        "        'detail': 'Measures variability in codon usage frequencies. High values suggest heterogeneity in codon preference across the genome.'\n",
        "    },\n",
        "    'skewness': {\n",
        "        'description': 'Skewness of Codon Usage',\n",
        "        'detail': 'Indicates asymmetry in codon usage distribution. Positive or negative skew can reflect selection pressure or mutational bias.'\n",
        "    },\n",
        "    'kurtosis': {\n",
        "        'description': 'Kurtosis of Codon Usage',\n",
        "        'detail': 'Reflects peakedness or flatness of codon usage distribution. May reveal codon usage extremity in specific genomes.'\n",
        "    },\n",
        "    'entropy_std_interaction': {\n",
        "        'description': 'Std. of Codon Entropy Interaction',\n",
        "        'detail': 'Standard deviation of entropy across codon-related interactions, capturing how stable or variable codon diversity is across species or genes.'\n",
        "    },\n",
        "    'gini': {\n",
        "        'description': 'Gini Index of Codon Usage',\n",
        "        'detail': 'Measures inequality in codon usage. Similar to entropy, but emphasizes skewed distribution. Higher Gini implies stronger codon preference.'\n",
        "    },\n",
        "    'polar_entropy': {\n",
        "        'description': 'Entropy of Polar Amino Acid Usage',\n",
        "        'detail': 'Quantifies diversity in the usage of codons coding for polar amino acids (e.g., Ser, Thr, Asn). Lower values may indicate specialization in protein interfaces.'\n",
        "    },\n",
        "    'special_entropy': {\n",
        "        'description': 'Entropy of Special Amino Acid Usage',\n",
        "        'detail': 'Measures the variation in codon usage for amino acids with unique properties (e.g., Proline, Glycine). Useful in identifying structural motifs.'\n",
        "    },\n",
        "    'polar_usage': {\n",
        "        'description': 'Usage of Polar Amino Acids',\n",
        "        'detail': 'Reflects how frequently polar amino acids are used. Polar residues are involved in solubility and hydrogen bonding.'\n",
        "    },\n",
        "    'special_usage': {\n",
        "        'description': 'Usage of Special Amino Acids',\n",
        "        'detail': 'Usage frequency for special amino acids like Glycine, Proline, and Cysteine, which affect protein flexibility and stability.'\n",
        "    },\n",
        "    'hydrophobic_entropy': {\n",
        "        'description': 'Entropy of Hydrophobic Amino Acids',\n",
        "        'detail': 'Measures diversity in usage of hydrophobic amino acids. Important for understanding membrane protein evolution.'\n",
        "    },\n",
        "    'cai_approx': {\n",
        "        'description': 'Codon Adaptation Index (Approx)',\n",
        "        'detail': 'Approximate measure of how well a gene’s codon usage matches that of highly expressed genes. High CAI suggests translational optimization.'\n",
        "    },\n",
        "    'enc_approx': {\n",
        "        'description': 'Effective Number of Codons (ENC)',\n",
        "        'detail': 'Estimates how evenly synonymous codons are used. Lower ENC values indicate stronger codon bias.'\n",
        "    },\n",
        "    'cos_sim_mam': {\n",
        "        'description': 'Cosine Similarity to Mammals',\n",
        "        'detail': 'Similarity of codon usage profile to mammalian species. Higher values suggest convergent evolutionary pressures or horizontal gene transfer.'\n",
        "    },\n",
        "    'cos_sim_vrt': {\n",
        "        'description': 'Cosine Similarity to Vertebrates',\n",
        "        'detail': 'Degree of similarity between species codon usage and vertebrates. Can be used to infer evolutionary proximity or codon mimicry.'\n",
        "    },\n",
        "    'cos_sim_vrl': {\n",
        "        'description': 'Cosine Similarity to Viruses (Large)',\n",
        "        'detail': 'Indicates how closely codon usage matches that of large viruses. Useful for detecting viral mimicry or co-evolution.'\n",
        "    },\n",
        "    'cos_sim_rod': {\n",
        "        'description': 'Cosine Similarity to Rodents',\n",
        "        'detail': 'Codon usage similarity to rodent species like mice. May indicate shared translational constraints or genomic mimicry.'\n",
        "    },\n",
        "    'cos_sim_plm': {\n",
        "        'description': 'Cosine Similarity to Plants (Monocots)',\n",
        "        'detail': 'Measures resemblance to monocotyledonous plant codon usage (e.g., rice, wheat). Useful in gene transfer and synthetic biology contexts.'\n",
        "    },\n",
        "    'cos_sim_pri': {\n",
        "        'description': 'Cosine Similarity to Primates',\n",
        "        'detail': 'Similarity score to primate codon usage profiles, such as human or chimpanzee.'\n",
        "    },\n",
        "    'cos_sim_phg': {\n",
        "        'description': 'Cosine Similarity to Phages',\n",
        "        'detail': 'Codon usage similarity to bacteriophages. High similarity may indicate phage infection or adaptation.'\n",
        "    },\n",
        "    'cos_sim_arc': {\n",
        "        'description': 'Cosine Similarity to Archaea',\n",
        "        'detail': 'Resemblance to codon usage patterns found in Archaea. May suggest horizontal gene transfer or extremophile adaptation.'\n",
        "    },\n",
        "    'cos_sim_bct': {\n",
        "        'description': 'Cosine Similarity to Bacteria',\n",
        "        'detail': 'Reflects how close codon usage is to bacterial species. Often high in prokaryotic genomes or symbiotic eukaryotes.'\n",
        "    },\n",
        "    'cos_sim_pln': {\n",
        "        'description': 'Cosine Similarity to Plants',\n",
        "        'detail': 'Similarity to general plant codon usage, encompassing both monocots and dicots.'\n",
        "    },\n",
        "    'cos_sim_inv': {\n",
        "        'description': 'Cosine Similarity to Invertebrates',\n",
        "        'detail': 'Used to evaluate alignment of codon preferences with invertebrate species.'\n",
        "    },\n",
        "    'cosine_similarity_to_median': {\n",
        "        'description': 'Cosine Similarity to Median Codon Profile',\n",
        "        'detail': 'How close the codon usage of a species is to the median profile across all species. Can reveal outlier behavior or typicality.'\n",
        "    },\n",
        "    'cosine_similarity_to_mean': {\n",
        "        'description': 'Cosine Similarity to Mean Codon Profile',\n",
        "        'detail': 'Measures alignment with average codon usage. Useful for detecting deviation from the global codon trend.'\n",
        "    },\n",
        "    'charged_positive_entropy': {\n",
        "        'description': 'Entropy of Positively Charged Amino Acid Codon Usage',\n",
        "        'detail': 'Captures diversity in codon usage for K, R, H. Lower entropy implies stronger bias, possibly due to translational optimization.'\n",
        "    },\n",
        "    'charged_negative_entropy': {\n",
        "        'description': 'Entropy of Negatively Charged Amino Acid Codon Usage',\n",
        "        'detail': 'Diversity measure for D, E codon usage. High entropy means more varied codon usage; low may indicate selection for efficiency.'\n",
        "    },\n",
        "    'A': {\n",
        "        'description': 'Alanine (A)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Alanine. Often found in alpha-helices and involved in protein flexibility.'\n",
        "    },\n",
        "    'C': {\n",
        "        'description': 'Cysteine (C)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Cysteine. Important for disulfide bonds and structural stability of proteins.'\n",
        "    },\n",
        "    'D': {\n",
        "        'description': 'Aspartic Acid (D)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Aspartic Acid. Negatively charged, often found in active sites or catalytic residues.'\n",
        "    },\n",
        "    'E': {\n",
        "        'description': 'Glutamic Acid (E)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Glutamic Acid. Negatively charged, involved in protein solubility and enzyme activity.'\n",
        "    },\n",
        "    'F': {\n",
        "        'description': 'Phenylalanine (F)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Phenylalanine. Aromatic and hydrophobic, contributes to protein core structure.'\n",
        "    },\n",
        "    'G': {\n",
        "        'description': 'Glycine (G)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Glycine. The smallest amino acid, often found in tight turns and flexible regions.'\n",
        "    },\n",
        "    'H': {\n",
        "        'description': 'Histidine (H)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Histidine. Positively charged at physiological pH and crucial in metal binding and catalysis.'\n",
        "    },\n",
        "    'I': {\n",
        "        'description': 'Isoleucine (I)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Isoleucine. Hydrophobic, frequently located in the interior of proteins.'\n",
        "    },\n",
        "    'K': {\n",
        "        'description': 'Lysine (K)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Lysine. Positively charged, important for interactions with DNA and other molecules.'\n",
        "    },\n",
        "    'L': {\n",
        "        'description': 'Leucine (L)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Leucine. A common hydrophobic residue found in protein cores.'\n",
        "    },\n",
        "    'M': {\n",
        "        'description': 'Methionine (M)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Methionine. The universal start codon, also involved in methyl group transfers.'\n",
        "    },\n",
        "    'N': {\n",
        "        'description': 'Asparagine (N)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Asparagine. Polar and often glycosylated in proteins.'\n",
        "    },\n",
        "    'P': {\n",
        "        'description': 'Proline (P)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Proline. Unique cyclic structure, often introduces kinks in protein chains.'\n",
        "    },\n",
        "    'Q': {\n",
        "        'description': 'Glutamine (Q)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Glutamine. Polar, involved in nitrogen metabolism and signaling.'\n",
        "    },\n",
        "    'R': {\n",
        "        'description': 'Arginine (R)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Arginine. Positively charged, often interacts with phosphate backbones and active sites.'\n",
        "    },\n",
        "    'S': {\n",
        "        'description': 'Serine (S)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Serine. Polar, frequently involved in phosphorylation and enzymatic functions.'\n",
        "    },\n",
        "    'T': {\n",
        "        'description': 'Threonine (T)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Threonine. Polar and often phosphorylated in signaling proteins.'\n",
        "    },\n",
        "    'V': {\n",
        "        'description': 'Valine (V)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Valine. Hydrophobic, typically buried inside proteins.'\n",
        "    },\n",
        "    'W': {\n",
        "        'description': 'Tryptophan (W)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Tryptophan. Aromatic, involved in protein-protein interactions and fluorescence.'\n",
        "    },\n",
        "    'Y': {\n",
        "        'description': 'Tyrosine (Y)',\n",
        "        'detail': 'Codon usage frequency for the amino acid Tyrosine. Aromatic and polar, often involved in signaling via phosphorylation.'\n",
        "    },\n",
        "    'median': {\n",
        "    'description': 'Median Codon Usage Value',\n",
        "    'detail': 'The median of codon usage frequencies across all codons for a species. Represents the central tendency and is less sensitive to outliers than the mean.'\n",
        "}\n",
        "}"
      ],
      "metadata": {
        "id": "x8ddpjpTW1Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_feature_metadata(df, feature_col='Feature',\n",
        "                         description_col='Feature Description',\n",
        "                         detail_col='Feature Detail',\n",
        "                         metadata_dict=None):\n",
        "    \"\"\"\n",
        "    Menambahkan deskripsi dan detail fitur ke dataframe berdasarkan dictionary metadata.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Dataframe yang ingin diperkaya.\n",
        "        feature_col (str): Nama kolom fitur yang ingin dipetakan.\n",
        "        description_col (str): Nama kolom output untuk deskripsi fitur.\n",
        "        detail_col (str): Nama kolom output untuk detail fitur.\n",
        "        metadata_dict (dict): Dictionary berisi mapping fitur ke 'description' dan 'detail'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Salinan dataframe dengan dua kolom baru: deskripsi dan detail fitur.\n",
        "    \"\"\"\n",
        "    if metadata_dict is None:\n",
        "        raise ValueError(\"metadata_dict harus diberikan!\")\n",
        "\n",
        "    df = df.copy()\n",
        "    df[description_col] = df[feature_col].map(\n",
        "        lambda x: metadata_dict.get(x, {}).get('description', x)\n",
        "    )\n",
        "    df[detail_col] = df[feature_col].map(\n",
        "        lambda x: metadata_dict.get(x, {}).get('detail', '')\n",
        "    )\n",
        "    return df"
      ],
      "metadata": {
        "id": "DhoFt2r-73VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df = map_feature_metadata(\n",
        "    importance_df,\n",
        "    feature_col='Feature',\n",
        "    metadata_dict=feature_description_full\n",
        ")"
      ],
      "metadata": {
        "id": "61ZBCgCQ76mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df"
      ],
      "metadata": {
        "id": "2p0VKnOEX6b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_result = pd.read_csv(\"hasil_clustering_clean.csv\")"
      ],
      "metadata": {
        "id": "hzNB4e-oYEak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_labeled_cluster_features(cluster_df, cluster_result, importance_df):\n",
        "    \"\"\"\n",
        "    Menghasilkan dataframe yang berisi fitur dominan per cluster dan kingdom,\n",
        "    lengkap dengan deskripsi, detail, dan importance-nya.\n",
        "\n",
        "    Parameters:\n",
        "        cluster_df (pd.DataFrame): Dataframe dengan kolom ['Cluster', 'Feature'].\n",
        "        cluster_result (pd.DataFrame): Dataframe dengan kolom ['Kingdom', 'Cluster'].\n",
        "        importance_df (pd.DataFrame): Dataframe dengan kolom ['Feature', 'Importance',\n",
        "                                                              'Feature Description', 'Feature Detail'].\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Dataframe akhir dengan informasi fitur dominan per kingdom-cluster,\n",
        "                      beserta detail deskriptif dan importance-nya.\n",
        "    \"\"\"\n",
        "    # Ambil fitur dominan per cluster\n",
        "    cluster_dominant_features = cluster_df.groupby(\"Cluster\")[\"Feature\"].apply(list).reset_index()\n",
        "\n",
        "    # Hitung jumlah spesies per kingdom-cluster\n",
        "    kingdom_clusters = cluster_result.groupby([\"Kingdom\", \"Cluster\"]).size().reset_index(name=\"n_species\")\n",
        "\n",
        "    # Merge untuk mendapatkan tabel awal\n",
        "    final_df = kingdom_clusters.merge(cluster_dominant_features, on=\"Cluster\", how=\"left\")\n",
        "\n",
        "    # Ekspand list fitur menjadi baris-baris terpisah\n",
        "    final_exploded = final_df.explode(\"Feature\")\n",
        "\n",
        "    # Join dengan importance_df untuk ambil metadata deskriptif\n",
        "    final_labeled = final_exploded.merge(\n",
        "        importance_df[[\"Feature\", \"Importance\", \"Feature Description\", \"Feature Detail\"]],\n",
        "        on=\"Feature\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "\n",
        "    return final_labeled"
      ],
      "metadata": {
        "id": "-t_lwWAVbjW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_labeled = extract_labeled_cluster_features(cluster_df, cluster_result, importance_df)\n",
        "final_labeled"
      ],
      "metadata": {
        "id": "sW9IvAhVo1If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_labeled.to_csv(\"kingdom_cluster_feature_insight.csv\")"
      ],
      "metadata": {
        "id": "BYob-oEqAoTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"kingdom_cluster_feature_insight.csv\")"
      ],
      "metadata": {
        "id": "oiOzar8I31Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"hasil_clustering_clean.csv\")"
      ],
      "metadata": {
        "id": "Ha_r4daJ4X0z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}